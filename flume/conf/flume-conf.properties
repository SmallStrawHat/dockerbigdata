
consumer.sources = s
consumer.channels = cElastic cHDFS
consumer.sinks = rElastic rHDFS

#source
consumer.sources.s.type = org.apache.flume.plugins.KafkaSource
consumer.sources.s.zookeeper.connect=zookeeper:2181
consumer.sources.s.group.id=testGroup
consumer.sources.s.zookeeper.session.timeout.ms=400
consumer.sources.s.zookeeper.sync.time.ms=200
consumer.sources.s.auto.commit.interval.ms=1000
consumer.sources.s.custom.topic.name=test
consumer.sources.s.custom.thread.per.consumer=10
#set channels
consumer.sources.s.channels = cElastic cHDFS

############################################
# Elastic config
###########################################

#sink
consumer.sinks.rElastic.type = org.apache.flume.sink.elasticsearch.ElasticSearchSink
#attention 's'
consumer.sinks.rElastic.hostNames = elasticsearch:9300
#require elasticsearch and lucene-core jars to FLUME_HOME/lib
consumer.sinks.rElastic.indexName = flume
consumer.sinks.rElastic.indexType = log
consumer.sinks.rElastic.batchSize = 500
consumer.sinks.rElastic.ttl = 5d
consumer.sinks.rElastic.serializer = org.apache.flume.sink.elasticsearch.ElasticSearchDynamicSerializer
consumer.sinks.rElastic.channel = cElastic

#channel
consumer.channels.cElastic.type = memory
consumer.channels.cElastic.capacity = 1000000
consumer.channels.cElastic.capacity = 1000000
consumer.channels.cElastic.transactionCapacity = 10000
consumer.channels.cElastic.byteCapacityBufferPercentage = 40
consumer.channels.cElastic.byteCapacity = 8000000


############################################
# HDFS config
###########################################

#sink
consumer.sinks.rHDFS.type = hdfs
consumer.sinks.rHDFS.hdfs.path = hdfs://hadoop:9000/flume/events/%y-%m-%d/%H%M/%S
#need commons-configuration-x.x.jar to FLUME_HOME/lib
consumer.sinks.rHDFS.hdfs.filePrefix = events-
consumer.sinks.rHDFS.hdfs.useLocalTimeStamp = true
consumer.sinks.rHDFS.hdfs.round = true
consumer.sinks.rHDFS.hdfs.roundValue = 10
consumer.sinks.rHDFS.hdfs.roundUnit = minute
consumer.sinks.rHDFS.hdfs.fileType = DataStream
consumer.sinks.rHDFS.hdfs.threadsPoolSize = 1000
consumer.sinks.rHDFS.hdfs.rollInterval = 100
consumer.sinks.rHDFS.hdfs.batchSize = 500
consumer.sinks.rHDFS.hdfs.maxOpenFiles = 50000
consumer.sinks.rHDFS.channel = cHDFS

#channel
consumer.channels.cHDFS.type = memory
consumer.channels.cHDFS.capacity = 1000000
consumer.channels.cHDFS.transactionCapacity = 10000
consumer.channels.cHDFS.byteCapacityBufferPercentage = 40
consumer.channels.cHDFS.byteCapacity = 8000000
